{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports\nThe following packages are imported:\n- tensorflow\n- matplotlib\n- numpy\n- IPython\n","metadata":{}},{"cell_type":"code","source":"# import wandb\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:31.228172Z","iopub.execute_input":"2021-11-22T12:32:31.228506Z","iopub.status.idle":"2021-11-22T12:32:36.028163Z","shell.execute_reply.started":"2021-11-22T12:32:31.228467Z","shell.execute_reply":"2021-11-22T12:32:36.027437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data\n- data_dir is the path to the images and the `results.csv`\n- image_dir is the path exculsively to the images\n- csv_file is the path to the `results.csv` file","metadata":{}},{"cell_type":"code","source":"data_dir = '../input/flickr-image-dataset/flickr30k_images'\nimage_dir = f'{data_dir}/flickr30k_images'\ncsv_file = f'{data_dir}/results.csv'","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.029844Z","iopub.execute_input":"2021-11-22T12:32:36.030548Z","iopub.status.idle":"2021-11-22T12:32:36.034643Z","shell.execute_reply.started":"2021-11-22T12:32:36.030502Z","shell.execute_reply":"2021-11-22T12:32:36.034066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we read the csv file as a dataframe and make some observations from it.\nFor a quick EDA we are going to \n- check the shape of the dataframe\n- check the names of the columns\n- find out the unique image names there are","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.035586Z","iopub.execute_input":"2021-11-22T12:32:36.036249Z","iopub.status.idle":"2021-11-22T12:32:36.444799Z","shell.execute_reply.started":"2021-11-22T12:32:36.036214Z","shell.execute_reply":"2021-11-22T12:32:36.444059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.447146Z","iopub.execute_input":"2021-11-22T12:32:36.447405Z","iopub.status.idle":"2021-11-22T12:32:36.588976Z","shell.execute_reply.started":"2021-11-22T12:32:36.447353Z","shell.execute_reply":"2021-11-22T12:32:36.588213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.590357Z","iopub.execute_input":"2021-11-22T12:32:36.590627Z","iopub.status.idle":"2021-11-22T12:32:36.651051Z","shell.execute_reply.started":"2021-11-22T12:32:36.590593Z","shell.execute_reply":"2021-11-22T12:32:36.650252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_size, val_size, test_size\n\ntrain_size = 60_000 \nval_size = 10_000\ntest_size = 20_000","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.652237Z","iopub.execute_input":"2021-11-22T12:32:36.652599Z","iopub.status.idle":"2021-11-22T12:32:36.659129Z","shell.execute_reply.started":"2021-11-22T12:32:36.652559Z","shell.execute_reply":"2021-11-22T12:32:36.658431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the dataframe accordingly","metadata":{}},{"cell_type":"code","source":"train_df = df.iloc[:train_size,:]\nval_df = df.iloc[train_size:train_size+val_size,:]\ntest_df = df.iloc[train_size+val_size:train_size+val_size+test_size,:]\n\ntrain_df.shape, val_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.661346Z","iopub.execute_input":"2021-11-22T12:32:36.661884Z","iopub.status.idle":"2021-11-22T12:32:36.670034Z","shell.execute_reply.started":"2021-11-22T12:32:36.661846Z","shell.execute_reply":"2021-11-22T12:32:36.669154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Enter different indices.\nindex = 200\n\nimage_name = train_df['image_name'][index]\ncomment = train_df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.671661Z","iopub.execute_input":"2021-11-22T12:32:36.671927Z","iopub.status.idle":"2021-11-22T12:32:36.700789Z","shell.execute_reply.started":"2021-11-22T12:32:36.671884Z","shell.execute_reply":"2021-11-22T12:32:36.700086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Handling\n- Defined the size of the vocab which is `5000`.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)","metadata":{}},{"cell_type":"code","source":"# Choose the top 10000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.701656Z","iopub.execute_input":"2021-11-22T12:32:36.70185Z","iopub.status.idle":"2021-11-22T12:32:36.706606Z","shell.execute_reply.started":"2021-11-22T12:32:36.701825Z","shell.execute_reply":"2021-11-22T12:32:36.705702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n\nIn the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value.","metadata":{}},{"cell_type":"code","source":"# build the vocabulary\ntokenizer.fit_on_texts(train_df['comment'].astype(\"str\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:36.709857Z","iopub.execute_input":"2021-11-22T12:32:36.710357Z","iopub.status.idle":"2021-11-22T12:32:37.741717Z","shell.execute_reply.started":"2021-11-22T12:32:36.710323Z","shell.execute_reply":"2021-11-22T12:32:37.74099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"pajama\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:37.74301Z","iopub.execute_input":"2021-11-22T12:32:37.743248Z","iopub.status.idle":"2021-11-22T12:32:37.748597Z","shell.execute_reply.started":"2021-11-22T12:32:37.743216Z","shell.execute_reply":"2021-11-22T12:32:37.747794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are padding the sentences so that each of the sentences are of the same length.","metadata":{}},{"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:37.749943Z","iopub.execute_input":"2021-11-22T12:32:37.750292Z","iopub.status.idle":"2021-11-22T12:32:37.756955Z","shell.execute_reply.started":"2021-11-22T12:32:37.750252Z","shell.execute_reply":"2021-11-22T12:32:37.756143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_df['comment'].astype(\"str\"))\nval_seqs = tokenizer.texts_to_sequences(val_df['comment'].astype(\"str\"))\ntest_seqs = tokenizer.texts_to_sequences(test_df['comment'].astype(\"str\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:37.758659Z","iopub.execute_input":"2021-11-22T12:32:37.759047Z","iopub.status.idle":"2021-11-22T12:32:39.1749Z","shell.execute_reply.started":"2021-11-22T12:32:37.759012Z","shell.execute_reply":"2021-11-22T12:32:39.174162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ntrain_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\nval_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\ntest_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:39.176304Z","iopub.execute_input":"2021-11-22T12:32:39.176597Z","iopub.status.idle":"2021-11-22T12:32:39.890741Z","shell.execute_reply.started":"2021-11-22T12:32:39.176562Z","shell.execute_reply":"2021-11-22T12:32:39.889993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caption vector\ntrain_cap_vector.shape, val_cap_vector.shape, test_cap_vector.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:39.8922Z","iopub.execute_input":"2021-11-22T12:32:39.892472Z","iopub.status.idle":"2021-11-22T12:32:39.89948Z","shell.execute_reply.started":"2021-11-22T12:32:39.892436Z","shell.execute_reply":"2021-11-22T12:32:39.898678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cap_ds = tf.data.Dataset.from_tensor_slices(train_cap_vector)\nval_cap_ds = tf.data.Dataset.from_tensor_slices(val_cap_vector)\ntest_cap_ds = tf.data.Dataset.from_tensor_slices(test_cap_vector)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:39.901259Z","iopub.execute_input":"2021-11-22T12:32:39.901917Z","iopub.status.idle":"2021-11-22T12:32:42.071877Z","shell.execute_reply.started":"2021-11-22T12:32:39.90188Z","shell.execute_reply":"2021-11-22T12:32:42.071169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Handling\n- Load the image\n- decode jpeg\n- resize\n- standardize","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef load_img(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.07454Z","iopub.execute_input":"2021-11-22T12:32:42.074953Z","iopub.status.idle":"2021-11-22T12:32:42.080512Z","shell.execute_reply.started":"2021-11-22T12:32:42.074916Z","shell.execute_reply":"2021-11-22T12:32:42.079791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_name = train_df['image_name'].values\nval_img_name = val_df['image_name'].values\ntest_img_name = test_df['image_name'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.081816Z","iopub.execute_input":"2021-11-22T12:32:42.082091Z","iopub.status.idle":"2021-11-22T12:32:42.091489Z","shell.execute_reply.started":"2021-11-22T12:32:42.082057Z","shell.execute_reply":"2021-11-22T12:32:42.090779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_ds = tf.data.Dataset.from_tensor_slices(train_img_name).map(load_img)\nval_img_ds = tf.data.Dataset.from_tensor_slices(val_img_name).map(load_img)\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_img_name).map(load_img)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.093138Z","iopub.execute_input":"2021-11-22T12:32:42.093322Z","iopub.status.idle":"2021-11-22T12:32:42.223313Z","shell.execute_reply.started":"2021-11-22T12:32:42.093299Z","shell.execute_reply":"2021-11-22T12:32:42.222549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Joint data","metadata":{}},{"cell_type":"code","source":"# prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 64\n\ntrain_ds = tf.data.Dataset.zip((train_img_ds, train_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.zip((val_img_ds, val_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.zip((test_img_ds, test_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.224453Z","iopub.execute_input":"2021-11-22T12:32:42.225087Z","iopub.status.idle":"2021-11-22T12:32:42.239245Z","shell.execute_reply.started":"2021-11-22T12:32:42.225051Z","shell.execute_reply":"2021-11-22T12:32:42.238613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sanity check for the division of datasets","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Some global variables\nEMBEDDING_DIM = 256\nVOCAB_SIZE = top_k+1\nUNITS = 512\nKERNEL = 64\nFEATURES = 2048","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.240984Z","iopub.execute_input":"2021-11-22T12:32:42.241464Z","iopub.status.idle":"2021-11-22T12:32:42.245937Z","shell.execute_reply.started":"2021-11-22T12:32:42.241419Z","shell.execute_reply":"2021-11-22T12:32:42.245139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using InceptionV3","metadata":{}},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dim, batch_size):\n        super(CNN_Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\n        self.new_input = self.image_model.input\n        self.hidden_layer = self.image_model.layers[-1].output\n        self.image_features_extract_model = tf.keras.Model(self.new_input, self.hidden_layer)\n        self.image_features_extract_model.trainable = False\n        \n        self.reshape = tf.keras.layers.Reshape(target_shape=(KERNEL,FEATURES))\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='relu')\n        \n    def call(self, x):\n        x = self.image_features_extract_model(x)\n        x = self.reshape(x)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.246879Z","iopub.execute_input":"2021-11-22T12:32:42.248505Z","iopub.status.idle":"2021-11-22T12:32:42.257419Z","shell.execute_reply.started":"2021-11-22T12:32:42.24847Z","shell.execute_reply":"2021-11-22T12:32:42.256674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the encoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\nfor image, caption in train_ds.take(1):\n    features = encoder(image)\n    print(f\"ENCODER OUTPUT: {features.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:42.258783Z","iopub.execute_input":"2021-11-22T12:32:42.259502Z","iopub.status.idle":"2021-11-22T12:32:52.351537Z","shell.execute_reply.started":"2021-11-22T12:32:42.259476Z","shell.execute_reply":"2021-11-22T12:32:52.35075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, annotations, hidden):\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        attention_hidden_layer = (tf.nn.tanh(self.W1(annotations) +\n                                             self.W2(hidden_with_time_axis)))\n        score = self.V(attention_hidden_layer)\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * annotations\n        context_vector = tf.reduce_sum(context_vector, axis=1) \n\n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:52.354533Z","iopub.execute_input":"2021-11-22T12:32:52.355075Z","iopub.status.idle":"2021-11-22T12:32:52.363227Z","shell.execute_reply.started":"2021-11-22T12:32:52.355031Z","shell.execute_reply":"2021-11-22T12:32:52.362497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size, batch_size):\n        super(RNN_Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.units = units\n\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.fc1 = tf.keras.layers.Dense(self.units)\n        self.fc2 = tf.keras.layers.Dense(vocab_size)\n        self.attention = BahdanauAttention(self.units)\n\n    def call(self, x, annotations, hidden):\n        context_vector, attention_weights = self.attention(annotations, hidden)\n        x = self.embedding(x)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        output, state = self.gru(x)\n        x = self.fc1(output)\n        x = tf.reshape(x, (-1, x.shape[2]))\n        x = self.fc2(x)\n\n        return x, state, attention_weights\n\n    def reset_state(self):\n        return tf.zeros((self.batch_size, self.units))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:52.364563Z","iopub.execute_input":"2021-11-22T12:32:52.365019Z","iopub.status.idle":"2021-11-22T12:32:52.378692Z","shell.execute_reply.started":"2021-11-22T12:32:52.364978Z","shell.execute_reply":"2021-11-22T12:32:52.377955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, BATCH_SIZE)\n\nfor image, caption in train_ds.take(1):\n    features = encoder(image)\n    print(f\"ENCODER OUTPUT: {features.shape}\")\n    hidden = decoder.reset_state()\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * caption.shape[0], 1)\n    predictions, hidden, attn_weights = decoder(dec_input, features, hidden)\n    print(f\"PREDICTION: {predictions.shape}\")\n    print(f\"HIDDEN: {hidden.shape}\")\n    print(f\"ATTENTION: {attn_weights.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:52.380124Z","iopub.execute_input":"2021-11-22T12:32:52.380664Z","iopub.status.idle":"2021-11-22T12:32:55.04382Z","shell.execute_reply.started":"2021-11-22T12:32:52.380628Z","shell.execute_reply":"2021-11-22T12:32:55.043045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wrapping the Gradient Tape in Model Class","metadata":{}},{"cell_type":"code","source":"for image, caption in train_ds.take(1):\n    print(image.shape)\n    print(caption.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:55.045117Z","iopub.execute_input":"2021-11-22T12:32:55.04536Z","iopub.status.idle":"2021-11-22T12:32:55.322395Z","shell.execute_reply.started":"2021-11-22T12:32:55.045325Z","shell.execute_reply":"2021-11-22T12:32:55.321671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Image_Caption_Gen(tf.keras.Model):\n    def __init__(self, encoder, decoder):\n        super(Image_Caption_Gen, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def train_step(self, data):\n        img_tensor, target = data\n        \n        loss = 0\n        \n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = self.decoder.reset_state()\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n        \n        with tf.GradientTape() as tape:\n            features = self.encoder(img_tensor)\n            \n            for i in range(1, target.shape[1]):\n                # passing the features through the decoder\n                predictions, hidden, _ = self.decoder(dec_input, features, hidden)\n                \n                loss += loss_function(target[:, i], predictions)\n                \n                # using teacher forcing\n                dec_input = tf.expand_dims(target[:, i], 1)\n                \n        total_loss = (loss / int(target.shape[1]))\n        trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        return {\"custom_loss\": total_loss}\n    \n    def test_step(self, data):\n        img_tensor, target = data\n        \n        loss = 0\n        \n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = self.decoder.reset_state()\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n        \n        features = self.encoder(img_tensor)\n            \n        for i in range(1, target.shape[1]):\n            # passing the features through the decoder\n            predictions, hidden, _ = self.decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n                \n        total_loss = (loss / int(target.shape[1]))\n        return {\"custom_loss\": total_loss}","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:55.323978Z","iopub.execute_input":"2021-11-22T12:32:55.324229Z","iopub.status.idle":"2021-11-22T12:32:55.337114Z","shell.execute_reply.started":"2021-11-22T12:32:55.324195Z","shell.execute_reply":"2021-11-22T12:32:55.336411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same.","metadata":{}},{"cell_type":"code","source":"# Early Stopping to prevent overfitting\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_custom_loss\", patience=2, verbose=2, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:55.338355Z","iopub.execute_input":"2021-11-22T12:32:55.339168Z","iopub.status.idle":"2021-11-22T12:32:55.348326Z","shell.execute_reply.started":"2021-11-22T12:32:55.339131Z","shell.execute_reply":"2021-11-22T12:32:55.347575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS=10\n# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, BATCH_SIZE)\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\nmain_model = Image_Caption_Gen(encoder, decoder)\nmain_model.compile(loss=loss_function, optimizer=optimizer)\n\nhistory_inception = main_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    callbacks = [es],\n    epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:32:55.349636Z","iopub.execute_input":"2021-11-22T12:32:55.34993Z","iopub.status.idle":"2021-11-22T12:33:19.235176Z","shell.execute_reply.started":"2021-11-22T12:32:55.349892Z","shell.execute_reply":"2021-11-22T12:33:19.233049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_test_loss = main_model.evaluate(test_ds)\nprint(f'[INFO] Test Loss: {custom_test_loss}')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:05:34.305405Z","iopub.execute_input":"2021-11-22T12:05:34.305845Z","iopub.status.idle":"2021-11-22T12:07:46.383381Z","shell.execute_reply.started":"2021-11-22T12:05:34.305807Z","shell.execute_reply":"2021-11-22T12:07:46.382515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now trying out Inception Resnet","metadata":{}},{"cell_type":"code","source":"EMBEDDING_DIM = 256\nVOCAB_SIZE = top_k+1\nUNITS = 512\nKERNEL = 64\nFEATURES = 2048","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:46.384985Z","iopub.execute_input":"2021-11-22T12:07:46.385316Z","iopub.status.idle":"2021-11-22T12:07:46.392459Z","shell.execute_reply.started":"2021-11-22T12:07:46.385276Z","shell.execute_reply":"2021-11-22T12:07:46.391637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef load_img_inception_resnet(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:46.3947Z","iopub.execute_input":"2021-11-22T12:07:46.395184Z","iopub.status.idle":"2021-11-22T12:07:46.403831Z","shell.execute_reply.started":"2021-11-22T12:07:46.395088Z","shell.execute_reply":"2021-11-22T12:07:46.403072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_ds_inception_resnet = tf.data.Dataset.from_tensor_slices(train_img_name).map(load_img_inception_resnet)\nval_img_ds_inception_resnet = tf.data.Dataset.from_tensor_slices(val_img_name).map(load_img_inception_resnet)\ntest_img_ds_inception_resnet = tf.data.Dataset.from_tensor_slices(test_img_name).map(load_img_inception_resnet)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:46.404832Z","iopub.execute_input":"2021-11-22T12:07:46.40508Z","iopub.status.idle":"2021-11-22T12:07:46.51186Z","shell.execute_reply.started":"2021-11-22T12:07:46.405044Z","shell.execute_reply":"2021-11-22T12:07:46.5111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds_inception_resnet = tf.data.Dataset.zip((train_img_ds_inception_resnet, train_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\nval_ds_inception_resnet = tf.data.Dataset.zip((val_img_ds_inception_resnet, val_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\ntest_ds_inception_resnet = tf.data.Dataset.zip((test_img_ds_inception_resnet, test_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:46.513966Z","iopub.execute_input":"2021-11-22T12:07:46.514527Z","iopub.status.idle":"2021-11-22T12:07:46.525261Z","shell.execute_reply.started":"2021-11-22T12:07:46.514487Z","shell.execute_reply":"2021-11-22T12:07:46.524378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KERNEL_RES = 64\nFEATURES_RES = 1536","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:07:46.526665Z","iopub.execute_input":"2021-11-22T12:07:46.527132Z","iopub.status.idle":"2021-11-22T12:07:46.534708Z","shell.execute_reply.started":"2021-11-22T12:07:46.527096Z","shell.execute_reply":"2021-11-22T12:07:46.533994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder_inception_resnet(tf.keras.Model):\n    \n    def __init__(self, embedding_dim, batch_size):\n        super(CNN_Encoder_inception_resnet, self).__init__()\n        self.batch_size = batch_size\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.image_model = tf.keras.applications.InceptionResNetV2(include_top=False,\n                                                weights='imagenet')\n        self.new_input = self.image_model.input\n        self.hidden_layer = self.image_model.layers[-1].output\n        self.image_features_extract_model = tf.keras.Model(self.new_input, self.hidden_layer)\n        self.image_features_extract_model.trainable = False\n        \n        self.reshape = tf.keras.layers.Reshape(target_shape=(KERNEL_RES,FEATURES_RES))\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='relu')\n        \n    def call(self, x):\n        x = self.image_features_extract_model(x)\n        x = self.reshape(x)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:53:48.588166Z","iopub.execute_input":"2021-11-22T06:53:48.58882Z","iopub.status.idle":"2021-11-22T06:53:48.604564Z","shell.execute_reply.started":"2021-11-22T06:53:48.588778Z","shell.execute_reply":"2021-11-22T06:53:48.601715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS=10\n# Test the decoder\nencoder = CNN_Encoder_inception_resnet(EMBEDDING_DIM, BATCH_SIZE)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, BATCH_SIZE)\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\nmain_model_res = Image_Caption_Gen(encoder, decoder)\nmain_model_res.compile(loss=loss_function, optimizer=optimizer)\n\nhistory_res = main_model_res.fit(\n    train_ds_inception_resnet,\n    validation_data=val_ds_inception_resnet,\n    callbacks = [es],\n    epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:53:52.420888Z","iopub.execute_input":"2021-11-22T06:53:52.421144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_inception.history[\"custom_loss\"], label=\"train_loss\")\nplt.plot(history_inception.history[\"val_custom_loss\"], label=\"val_loss\")\nplt.title(\"Loss vs. Epoch\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\n\nplt.savefig(\"loss.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:19:09.516801Z","iopub.execute_input":"2021-11-22T09:19:09.517138Z","iopub.status.idle":"2021-11-22T09:19:09.879619Z","shell.execute_reply.started":"2021-11-22T09:19:09.517093Z","shell.execute_reply":"2021-11-22T09:19:09.878372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the weights of the model for better reproducibility\nmain_model.encoder.save_weights(\"encoder_inception.h5\")\nmain_model.decoder.save_weights(\"decoder_inception.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T09:15:56.115968Z","iopub.execute_input":"2021-11-22T09:15:56.116271Z","iopub.status.idle":"2021-11-22T09:15:57.078355Z","shell.execute_reply.started":"2021-11-22T09:15:56.116241Z","shell.execute_reply":"2021-11-22T09:15:57.077288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_model_res.encoder.save_weights(\"encoder_res.h5\")\nmain_model_res.decoder.save_weights(\"decoder_res.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Captions","metadata":{}},{"cell_type":"code","source":"# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, 1)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, 1)\n\nfor image, caption in train_ds.take(1):\n    features = encoder(tf.expand_dims(image[1],0))\n    print(f\"ENCODER OUTPUT: {features.shape}\")\n    hidden = decoder.reset_state()\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 1)\n    predictions, hidden, attn_weights = decoder(dec_input, features, hidden)\n    print(f\"PREDICTION: {predictions.shape}\")\n    print(f\"HIDDEN: {hidden.shape}\")\n    print(f\"ATTENTION: {attn_weights.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:33:31.529786Z","iopub.execute_input":"2021-11-22T12:33:31.530039Z","iopub.status.idle":"2021-11-22T12:33:34.386127Z","shell.execute_reply.started":"2021-11-22T12:33:31.530009Z","shell.execute_reply":"2021-11-22T12:33:34.385418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.load_weights(\"../input/weight/encoder_inception.h5\")\ndecoder.load_weights(\"../input/weight/decoder_inception.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:33:34.388028Z","iopub.execute_input":"2021-11-22T12:33:34.388277Z","iopub.status.idle":"2021-11-22T12:33:38.054036Z","shell.execute_reply.started":"2021-11-22T12:33:34.38824Z","shell.execute_reply":"2021-11-22T12:33:38.05331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(image):\n    #                          max_length  64\n    attention_plot = np.zeros((64, KERNEL)) ## Kernel(depends upon encoder), this decides the size of attention_plot[i]\n\n    hidden = decoder.reset_state() ## initialization decoder\n\n    img = tf.expand_dims(load_img(image), 0) \n    features = encoder(img)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(64):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:48:42.200798Z","iopub.execute_input":"2021-11-22T12:48:42.201064Z","iopub.status.idle":"2021-11-22T12:48:42.211131Z","shell.execute_reply.started":"2021-11-22T12:48:42.201034Z","shell.execute_reply":"2021-11-22T12:48:42.210339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(20, 20)) # net figure size\n    len_result = len(result)\n    for i in range(len_result):\n        temp_att = np.resize(attention_plot[i], (8, 8))\n        print(temp_att)\n        ax = fig.add_subplot(len_result//2, len_result//2, i+1)\n        ax.set_title(result[i])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='copper_r', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:22.450239Z","iopub.execute_input":"2021-11-22T12:59:22.45098Z","iopub.status.idle":"2021-11-22T12:59:22.459673Z","shell.execute_reply.started":"2021-11-22T12:59:22.450941Z","shell.execute_reply":"2021-11-22T12:59:22.458197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:22.698643Z","iopub.execute_input":"2021-11-22T12:59:22.699125Z","iopub.status.idle":"2021-11-22T12:59:22.703071Z","shell.execute_reply.started":"2021-11-22T12:59:22.699089Z","shell.execute_reply":"2021-11-22T12:59:22.702286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_url = 'https://media.istockphoto.com/photos/happy-kids-playing-with-garden-sprinkler-picture-id1159180335'\nimage_extension = image_url[-4:]\nimage_path = tf.keras.utils.get_file('image'+image_extension,\n                                     origin=image_url)\n\nresult, attention_plot = evaluate(image_path)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image_path, result, attention_plot)\n# opening the image\nImage.open(image_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:23.044067Z","iopub.execute_input":"2021-11-22T12:59:23.044303Z","iopub.status.idle":"2021-11-22T12:59:27.5087Z","shell.execute_reply.started":"2021-11-22T12:59:23.044277Z","shell.execute_reply":"2021-11-22T12:59:27.507829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:11.757953Z","iopub.status.idle":"2021-11-22T12:59:11.758753Z","shell.execute_reply.started":"2021-11-22T12:59:11.758494Z","shell.execute_reply":"2021-11-22T12:59:11.758523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(70000, 90000)\n\n\nimage_name = train_df['image_name'][index]\ncomment = train_df['comment'][index]\n\nimage = test_df['image_name'][rid]\n\nreal_caption = test_df['comment'][rid]\nresult, attention_plot = evaluate(image)\n\n# remove <start> and <end> from the real_caption\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\n#remove \"<unk>\" in result\nfor i in result:\n   if i==\"<unk>\":\n       result.remove(i)\n\nfor i in real_caption:\n   if i==\"<unk>\":\n       real_caption.remove(i)\n\n#remove <end> from result        \nresult_join = ' '.join(result)\nresult_final = result_join.rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = result\n\nscore_BLEU1 = sentence_bleu(reference, candidate, weights=(1.0, 0.0, 0.0, 0.0))\nscore_BLEU2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0.0, 0.0))\nscore_BLEU3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0.0))\nscore_BLEU4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\nprint(f\"BELU-1 score: {score_BLEU1*100}\")\nprint(f\"BELU-2 score: {score_BLEU2*100}\")\nprint(f\"BELU-3 score: {score_BLEU3*100}\")\nprint(f\"BELU-4 score: {score_BLEU4*100}\")\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', result_final)\n\nplot_attention(image, result, attention_plot)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:59:11.760164Z","iopub.status.idle":"2021-11-22T12:59:11.760601Z","shell.execute_reply.started":"2021-11-22T12:59:11.760353Z","shell.execute_reply":"2021-11-22T12:59:11.76039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}